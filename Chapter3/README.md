# 제 3고지

## 25단계. 계산 그래프 시각화(1)
- Graphviz 사용
- `brew install graphviz`
- DOT 언어 배우기
- `digraph g{ \ x \ y \ }` 구조
- 각 줄마다 노드에 대한 정보가 담김
- 변수를 원, 함수를 사각형으로 표현

## 26단계. 계산 그래프 시각화(2)
- dezero/utils.py에 계산 그래프 시각화 함수 구현
- `_dot_var` 함수: 보조 함수, `get_dot_graph` 함수 전용으로 사용할 예정
    - Variable 인스턴스를 건네면 인스턴스 내용을 DOT 언어로 작성된 문자열로 바꿔서 반환
- `_dot_func` : dezero 함수를 DOT 언어로 변환
    - 함수와 입력 변수의 관계, 함수와 출력 변수의 관계 또한 DOT 언어로 기술
- `get_dot_graph` : Variable 클래스의 backward 메서드와 거의 같으나, 미분값을 전파하는게 아니라 DOT 언어로 기술한 문자열을 txt에 추가
- generation 값으로 정렬하는 코드는 주석처리 (노드의 추적 순서는 중요하지 않음)
- `plot_dot_graph` : 이미지 변환까지 한번에 수행
    - `os.path.expanduser('~')`: 사용자의 홈 디렉토리를 뜻하는 ~를 절대 경로로 풀어줌
    - `to_file`에 저장할 이미지 파일의 이름 지정

- 이 챕터의 아쉬운 점: 페이지가 바뀔 때 indentation 표기에 대해 더 신경써줬으면..

## 27단계. 테일러 급수 미분
- sin 함수 구현
- 테일러 급수: 어떤 함수를 다항식으로 근사하는 방법
- 매클로린 전개: a=0일 때의 테일러 급수
- threshold를 작게 할 수록 이론상 근사 정밀도가 좋아지지만, 컴퓨터의 계산에서는 자릿수 누락 혹은 반올림이 발생하므로 반드시 이론과 일치하지는 않음

## 28단계. 함수 최적화
- 최적화: 어떤 함수가 주어졌을 때 그 최솟값 또는 최댓값을 반환하는 '입력'을 찾는 일
- 신경망 학습의 목표: 손실 함수의 출력을 최소화하는 매개변수를 찾는 일
- dezero를 사용하여 로젠브록 함수의 출력이 최소가 되는 x0, x1 찾기
- 경사하강법 활용
- iteration을 늘릴수록 원하는 최적화 값에 도달할 수 있음

## 29단계. 방법으로 푸는 최적화(수동 계산)
- 경사하강법 단점: 일반적으로 수렴이 느림
- 대체 방법: 뉴턴 방법
- 초깃값이 정답에 충분히 가까우면 뉴턴 방법이 더 빨리 수렴
- 2차 미분의 정보를 이용해 효율적인 탐색을 가능하게 함
- dezero에서 2차 미분 자동으로 구할수 없으므로 수동으로 계산
- $ y= x^4 - 2x^2 $ 의 2차 미분 구하기

## 30단계. 고차미분 (준비 편)
- dezero는 1차 미분까지만 자동 계산
- 고차 미분을 자동으로 할 수 있도록 확장
### 복습
- Variable 인스턴스 변수
    - init 메서드: `self.data`와 `self.grad` 는 각각 순전파 계산, 역전파 계산시 사용되며 ndarray를 저장
- Function 클래스
    - Variable 인스턴스 변수 data를 꺼내 리스트 xs로 모은 뒤 `forward(*xs)` 를 호출해 계산 수행
    - Variable 과 Function 간 관계 생성
    - Dezero의 함수는 모두 Function 클래스 상속
- Variable 클래스의 역전파
    - Variable 인스턴스 변수 grad를 리스트로 모음 (grad: ndarray 인스턴스 참조 중)
    - `backward` 메서드에 ndarray 인스턴스가 담긴 리스트가 전달
    - 출력 쪽에서 전파하는 미분값 gxs를 함수의 입력 변수의 grad로 설정
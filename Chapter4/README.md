# 제 4고지

## 37단계. 텐서를 다루다
- 머신러닝 데이터는 벡터나 행렬 등의 '텐서'를 주로 사용
- 지금까지 구현한 함수들에 텐서를 입력하면 원소 별로 적용, 같은 차원의 결과를 얻음 
- 야코비 행렬
- 텐서의 미분=기울기
- 행렬 곱을 계산하는 순서 
    - 입력->출력 방향 = 자동 미분의 forward 모드
    - 출력->입력 방향 = reverse 모드
- 원소별 미분을 계산한 다음 결괏값을 곱하면 최종 결과를 얻을 수 있음 = 효율적 계산

## 38단계. 형상 변환 함수
- 원소별로 계산하지 않는 함수: reshape, transpose
- 텐서의 형상을 고려해야 함
- Variable 클래스에서 reshape을 사용할 수 있도록 함수 추가
- 행렬의 전치 

## 39단계. 합계 함수
- 합계를 구하는 함수 sum을 추가하고 sum 함수의 미분 
- 원소가 2개 이상인 벡터의 합에 대한 역전파: 기울기 벡터의 원소 수만큼 복사
- 넘파이의 `np.sum`은 합계를 구할 때 축 지정 가능
- axis=0이면 세로 방향 합, axis=1이면 가로 방향 합을 구함
- `keepdims=True`로 지정하면 차원의 수를 유지

## 40단계. 브로드캐스트 함수
- broadcast_to 함수 구현 및 dezero에서 넘파이처럼 브로드캐스트하도록 수정
- 원소 복사가 일어나면 기울기를 합하면 됨
- `np.broadcast_to(x, shape)` : ndarray의 인스턴스인 x의 원소를 복제, shape 인수로 지정한 형상이 되도록 함
- 역전파에서는 입력 x 와 형상이 같아지도록 broadcast_to 와 sum_to 함수가 상호 의존적

## 41단계. 행렬의 곱
- 벡터의 내적: 두 벡터 사이의 대응 원소의 곱을 모두 합한 값
- 행렬의 곱: 왼쪽 행렬의 가로 방향 벡터와 오른쪽 행렬의 세로 방향 벡터의 내적 계산
- 행렬곱 형상: (a,b) * (c,d) = (a,d)

## 42단계. 선형 회귀
- 머신러닝은 데이터를 사용해 문제를 해결
- 모아진 데이터로부터 컴퓨터가 스스로 해법을 찾아냄
- 선형 회귀의 목적: 데이터에 맞는 $ y=Wx+b $ 식 찾기
- 잔차(residual) 최소화 = 손실 함수의 출력을 최소화하는 $ W $, $ b $ 찾기

## 43단계. 신경망
- dezero의 linear 함수 구현
    - 1. DeZero 함수를 사용해 구현
    - 2. Function 클래스를 상속하여 새롭게 Linear 함수 클래스 구현 -> 메모리를 효율적으로 사용 
- 신경망에서 선형 변환 = 완전 연결 계층 (fully connected layer)
- 매개변수 W = 가중치 (weight)
- 매개변수 b = 편향 (bias)
- 신경망은 비선형 데이터셋에 효과적
- 활성화 함수: 선형 변환의 출력에 비선형 변환 수행 e.g ReLU, sigmoid
- sigmoid 함수를 `dezero/functions.py`에서 구현하면 메모리 효율 좋아짐
- 일반적인 신경망: 선형 변환 - 활성화 함수 - 선형 변환 - 활성화 함수
- 층 (layer): 선형 변환이나 활성화 함수 등에 의한 변환

## 44단계. 매개변수를 모아두는 계층
- parameter 클래스 구현
- layer 클래스 구현
    - 변수를 변환하는 클래스, But 매개변수를 유지함
    - layer 인스턴스 변수를 설정하면 `Parameter` 인스턴스를 보유하고 있는 인스턴스 변수 이름만 `layer._params`에 추가됨
- yield: 처리를 일시 중지하고 값을 반환, 작업 재개 가능
- Linear 클래스의 개선: 가중치 발생 시험을 forward 메서드에서 생성, 입력 크기를 자동으로 결정

## 45단계. 계층을 모아두는 계층
- 여러 층의 신경망을 구현하기 위한 클래스 확장
- Layer 클래스는 여러 개의 parameter를 가질 수 있음
- Layer 클래스가 다른 layer도 담을 수 있게 확장
- 모델 = 신경망 = 수식으로 표현할 수 있는 함수
- `Model` 클래스는 Layer를 상속
- 범용적인 신경망 구성을 위해 MLP 클래스 구현
- 인스턴스 변수 설정을 `setattr` 함수로 하고 있음 

## 46단계. Optimizer로 수행하는 매개변수 갱신
- 경사하강법 외 다양한 최적화 기법이 있음
- 매개변수 갱신 작업 모듈화
- `class Optimizer`
    - `targer`, `hooks` 두 개 인스턴스 변수 초기화
    - `setup`: 매개변수를 갖는 클래스를 인스턴스 변수인 target으로 설정
    - `update`: 모든 매개변수 갱신, grad가 None 인 매개변수는 갱신 건너뜀
    - 매개변수 갱신에 앞서 전체 매개변수를 전처리 
- `class SGD`
    - Optimizer 클래스 상속
    - `__init__` 메서드에서 학습률 받아 초기화, 
    - `update_one` 메서드에서 매개변수 갱신 코드 구현
- MLP 클래스를 사용해 모댈 생성, SGD 클래스로 매개변수 갱신
- 기울기를 이용한 최적화 기법: Momentum, AdaGrad, AdaDelta, Adam

## 47단계. 소프트맥스 함수와 교차 엔트로피 오차
- 다중 클래스 분류: 새로운 유형의 문제
- 사전 준비 1. 슬라이스 조작 함수
    - 슬라이스로 인한 계산: 다차원 배열의 데이터 일부를 수정하지 않고 전달
    - `get_item`: 인덱스를 반복 지정, 동일한 원소를 여러번 뺄 수 있음
- 사전 준비 2. 소프트맥스

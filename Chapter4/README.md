# 제 4고지

## 37단계. 텐서를 다루다
- 머신러닝 데이터는 벡터나 행렬 등의 '텐서'를 주로 사용
- 지금까지 구현한 함수들에 텐서를 입력하면 원소 별로 적용, 같은 차원의 결과를 얻음 
- 야코비 행렬
- 텐서의 미분=기울기
- 행렬 곱을 계산하는 순서 
    - 입력->출력 방향 = 자동 미분의 forward 모드
    - 출력->입력 방향 = reverse 모드
- 원소별 미분을 계산한 다음 결괏값을 곱하면 최종 결과를 얻을 수 있음 = 효율적 계산

## 38단계. 형상 변환 함수
- 원소별로 계산하지 않는 함수: reshape, transpose
- 텐서의 형상을 고려해야 함
- Variable 클래스에서 reshape을 사용할 수 있도록 함수 추가
- 행렬의 전치 

## 39단계. 합계 함수
- 합계를 구하는 함수 sum을 추가하고 sum 함수의 미분 
- 원소가 2개 이상인 벡터의 합에 대한 역전파: 기울기 벡터의 원소 수만큼 복사
- 넘파이의 `np.sum`은 합계를 구할 때 축 지정 가능
- axis=0이면 세로 방향 합, axis=1이면 가로 방향 합을 구함
- `keepdims=True`로 지정하면 차원의 수를 유지

## 40단계. 브로드캐스트 함수
- broadcast_to 함수 구현 및 dezero에서 넘파이처럼 브로드캐스트하도록 수정
- 원소 복사가 일어나면 기울기를 합하면 됨
- `np.broadcast_to(x, shape)` : ndarray의 인스턴스인 x의 원소를 복제, shape 인수로 지정한 형상이 되도록 함
- 역전파에서는 입력 x 와 형상이 같아지도록 broadcast_to 와 sum_to 함수가 상호 의존적

## 41단계. 행렬의 곱
- 벡터의 내적: 두 벡터 사이의 대응 원소의 곱을 모두 합한 값
- 행렬의 곱: 왼쪽 행렬의 가로 방향 벡터와 오른쪽 행렬의 세로 방향 벡터의 내적 계산
- 행렬곱 형상: (a,b) * (c,d) = (a,d)

## 42단계. 선형 회귀
- 머신러닝은 데이터를 사용해 문제를 해결
- 모아진 데이터로부터 컴퓨터가 스스로 해법을 찾아냄
- 선형 회귀의 목적: 데이터에 맞는 $ y=Wx+b $ 식 찾기
- 잔차(residual) 최소화 = 손실 함수의 출력을 최소화하는 $ W $, $ b $ 찾기
